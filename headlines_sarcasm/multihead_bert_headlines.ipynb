{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc7a5c4",
   "metadata": {},
   "source": [
    "## Dependencies and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b39345",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ImportError as e:\n",
    "    print('transformers not installed')\n",
    "    print('Installing now...')\n",
    "    !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "    print(\"Install complete.\")\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530fec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import io \n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import transformers\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.utils.dummy_pt_objects import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForTokenClassification,AutoConfig,AutoModel,AutoTokenizer,BertModel,BertConfig,AdamW, get_constant_schedule,BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using Google Colab, set colab = True\n",
    "colab = False\n",
    "\n",
    "if colab == True:\n",
    "    #Mounting Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    \n",
    "    %cd '/content/gdrive/Shareddrives/523 Project/Data'\n",
    "    %ls\n",
    "else:\n",
    "    DATA_DIR = '/projectnb2/dl523/students/kjv/EC523_Project/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ed7ee",
   "metadata": {},
   "source": [
    "## Define Model Class and Training, Validation, and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multihead_attn_bert(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_encoder, embed_dim, num_attn_layers, num_heads):\n",
    "        super(multihead_attn_bert, self).__init__()\n",
    "        \n",
    "        self.bert = bert_encoder\n",
    "        \n",
    "        self.multiheads = nn.ModuleList([nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)])\n",
    "        self.multiheads.extend([nn.MultiheadAttention(embed_dim, num_heads, batch_first=True) for i in range(num_attn_layers-1)])\n",
    "        \n",
    "        self.GRU = nn.GRU(input_size=embed_dim, hidden_size=512, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # 1024 = hidden_size of GRU x 2 (for bidirectionality of GRU)\n",
    "        self.fc = nn.Linear(in_features=1024, out_features=1)\n",
    "        \n",
    "        # use dropout set of 0.2 as in paper\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokenized_input_values, attention_mask):\n",
    "        \n",
    "        output = self.bert(tokenized_input_values, attention_mask=attention_mask).last_hidden_state\n",
    "        \n",
    "        for multihead_layer in self.multiheads:\n",
    "            output,_ = multihead_layer(query=output,key=output,value=output,key_padding_mask=(~attention_mask.bool()))\n",
    "        \n",
    "        _,hidden = self.GRU(output)\n",
    "        \n",
    "        # concatenate bidirectional outputs from GRU to pass to linear layer\n",
    "        hidden = torch.cat([hidden[0,:, :], hidden[1,:,:]], dim=1).unsqueeze(0)\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387686d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training, testing, and validation loss functions for headlines data\n",
    "\n",
    "def train_mh_bert_headlines(model, trainloader, validationloader, optimizer, criterion, num_epochs, scheduler=None):\n",
    "        \n",
    "    avg_val_losses = []\n",
    "    avg_training_losses = []\n",
    "    epochs_finished = []    \n",
    "    \n",
    "    # conditions for early stopping\n",
    "    last_val_loss = float('inf')\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    es_counter = 0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        model.train()\n",
    "        if scheduler != None:\n",
    "            print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "\n",
    "        running_loss = 0\n",
    "        curr_total_train_loss = 0\n",
    "        print('Epoch: ',epoch)\n",
    "\n",
    "        for idx, (inputs,attn_mask,labels) in enumerate(tqdm(trainloader,total = len(trainloader))):\n",
    "\n",
    "            inputs, attn_mask, labels = inputs.to(device), attn_mask.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = mh_sarcasm_model(inputs, attn_mask)\n",
    "            output = torch.flatten(output)\n",
    "\n",
    "            # convert label type from int to float for use in BCELoss\n",
    "            labels = labels.float()\n",
    "            loss = criterion(output,labels)\n",
    "            curr_total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss every 100 batches\n",
    "            if idx % 100 == 0:\n",
    "                print('Loss: ',float(loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epochs_finished.append(epoch)\n",
    "        avg_training_losses.append(curr_total_train_loss/len(trainloader))\n",
    "        \n",
    "        # adjust scheduler after every epoch\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # check for changes in avg validation loss to determine if early stopping is needed\n",
    "        print(\"Checking validation loss...\")\n",
    "        curr_val_loss = validation_loss_headlines(model, validationloader)\n",
    "        avg_val_losses.append(curr_val_loss)\n",
    "        print(\"Average validation loss after last epoch: \", curr_val_loss)\n",
    "\n",
    "        if curr_val_loss > last_val_loss:\n",
    "            es_counter += 1\n",
    "\n",
    "            if es_counter >= patience:\n",
    "                print(\"Early stopping triggered. Ending training..\")\n",
    "                \n",
    "                # plot training and validation losses\n",
    "                plt.plot(epochs_finished, avg_training_losses, label = \"Training Loss\")\n",
    "                plt.plot(epochs_finished, avg_val_losses, label = \"Validation Loss\")\n",
    "                plt.title(\"Training and Validation Loss: Multihead Self-Attention Model\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.legend()\n",
    "                return\n",
    "            else:\n",
    "                print(f\"Increase in validation loss! {patience-es_counter} more consecutive loss increase(s) until early stop.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Decrease in validation loss. Early stop counter reset to 0.\")\n",
    "            es_counter = 0\n",
    "\n",
    "        last_val_loss = curr_val_loss\n",
    "\n",
    "        # check to save model if validation loss is lower than min recorded validation loss\n",
    "        if curr_val_loss < min_val_loss:\n",
    "            print(\"New best validation loss - saving model.\")\n",
    "            min_val_loss = curr_val_loss\n",
    "            save_dir = \"/projectnb/dl523/students/kjv/EC523_Project/Saved_Models/Multihead_BERT/trained_MH_BERT_headlines.pth\"\n",
    "            torch.save(model.state_dict(), save_dir)\n",
    "            \n",
    "    # plot training and validation losses\n",
    "    plt.plot(epochs_finished, avg_training_losses, label = \"Training Loss\")\n",
    "    plt.plot(epochs_finished, avg_val_losses, label = \"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss: Multihead Self-Attention Model\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "def test_mh_bert_headlines(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            \n",
    "            inputs, attn_mask, labels = data\n",
    "            inputs, attn_mask, labels = inputs.to(device), attn_mask.to(device), labels.to(device)\n",
    "            \n",
    "            output = torch.flatten(model(inputs, attn_mask))\n",
    "            \n",
    "            # convert output to class predictions\n",
    "            output[output<0.5] = 0\n",
    "            output[output>=0.5] = 1\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (output==labels).float().sum().item()\n",
    "            \n",
    "        acc = correct/total * 100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def validation_loss_headlines(model, validationloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validationloader:\n",
    "            \n",
    "            inputs, attn_mask, labels = data\n",
    "            inputs, attn_mask, labels = inputs.to(device), attn_mask.to(device), labels.to(device)\n",
    "            labels = labels.float()\n",
    "            \n",
    "            output = torch.flatten(model(inputs, attn_mask))\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss/len(validationloader)\n",
    "    \n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e623fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training, testing, and validation loss functions for Reddit data\n",
    "\n",
    "def train_mh_bert_reddit(model, trainloader, validationloader, optimizer, criterion, num_epochs, scheduler=None):\n",
    "        \n",
    "    avg_val_losses = []\n",
    "    avg_training_losses = []\n",
    "    epochs_finished = []    \n",
    "    \n",
    "    # conditions for early stopping\n",
    "    last_val_loss = float('inf')\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    es_counter = 0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        model.train()\n",
    "        if scheduler != None:\n",
    "            print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "\n",
    "        running_loss = 0\n",
    "        curr_total_train_loss = 0\n",
    "        print('Epoch: ',epoch)\n",
    "\n",
    "        for idx, (encodings, labels) in enumerate(tqdm(trainloader,total = len(trainloader))):\n",
    "\n",
    "            inputs = encodings['input_ids']\n",
    "            attn_mask = encodings['attention_mask']\n",
    "            inputs, attn_mask, labels = inputs.to(device), attn_mask.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = mh_sarcasm_model(inputs, attn_mask)\n",
    "            output = torch.flatten(output)\n",
    "\n",
    "            # convert label type from int to float for use in BCELoss\n",
    "            labels = labels.float()\n",
    "            loss = criterion(output,labels)\n",
    "            curr_total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss every 100 batches\n",
    "            if idx % 100 == 0:\n",
    "                print('Loss: ',float(loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epochs_finished.append(epoch)\n",
    "        avg_training_losses.append(curr_total_train_loss/len(trainloader))\n",
    "        \n",
    "        # adjust scheduler after every epoch\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # check for changes in avg validation loss to determine if early stopping is needed\n",
    "        print(\"Checking validation loss...\")\n",
    "        curr_val_loss = validation_loss_reddit(model, validationloader)\n",
    "        avg_val_losses.append(curr_val_loss)\n",
    "        print(\"Average validation loss after last epoch: \", curr_val_loss)\n",
    "\n",
    "        if curr_val_loss > last_val_loss:\n",
    "            es_counter += 1\n",
    "\n",
    "            if es_counter >= patience:\n",
    "                print(\"Early stopping triggered. Ending training..\")\n",
    "                \n",
    "                # plot training and validation losses\n",
    "                plt.plot(epochs_finished, avg_training_losses, label = \"Training Loss\")\n",
    "                plt.plot(epochs_finished, avg_val_losses, label = \"Validation Loss\")\n",
    "                plt.title(\"Training and Validation Loss: Multihead Self-Attention Model\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.legend()\n",
    "                return\n",
    "            else:\n",
    "                print(f\"Increase in validation loss! {patience-es_counter} more consecutive loss increase(s) until early stop.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Decrease in validation loss. Early stop counter reset to 0.\")\n",
    "            es_counter = 0\n",
    "\n",
    "        last_val_loss = curr_val_loss\n",
    "\n",
    "        # check to save model if validation loss is lower than min recorded validation loss\n",
    "        if curr_val_loss < min_val_loss:\n",
    "            print(\"New best validation loss - saving model.\")\n",
    "            min_val_loss = curr_val_loss\n",
    "            save_dir = \"/projectnb/dl523/students/kjv/EC523_Project/Saved_Models/Multihead_BERT/trained_MH_BERT_reddit.pth\"\n",
    "            torch.save(model.state_dict(), save_dir)\n",
    "            \n",
    "    # plot training and validation losses\n",
    "    plt.plot(epochs_finished, avg_training_losses, label = \"Training Loss\")\n",
    "    plt.plot(epochs_finished, avg_val_losses, label = \"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss: Multihead Self-Attention Model\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "def test_mh_bert_reddit(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for encodings, labels in testloader:\n",
    "            \n",
    "            inputs = encodings['input_ids']\n",
    "            attn_mask = encodings['attention_mask']\n",
    "            inputs, attn_mask, labels = inputs.to(device), attn_mask.to(device), labels.to(device)\n",
    "            \n",
    "            output = torch.flatten(model(inputs, attn_mask))\n",
    "            \n",
    "            # convert output to class predictions\n",
    "            output[output<0.5] = 0\n",
    "            output[output>=0.5] = 1\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (output==labels).float().sum().item()\n",
    "            \n",
    "        acc = correct/total * 100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def validation_loss_reddit(model, validationloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for encodings, labels in validationloader:\n",
    "            \n",
    "            inputs = encodings['input_ids']\n",
    "            attn_mask = encodings['attention_mask']\n",
    "            inputs, attn_mask, labels = inputs.to(device), attn_mask.to(device), labels.to(device)\n",
    "            labels = labels.float()\n",
    "            \n",
    "            output = torch.flatten(model(inputs, attn_mask))\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss/len(validationloader)\n",
    "    \n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903b7b9",
   "metadata": {},
   "source": [
    "## Model Initialization and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pre-trained BERT\n",
    "\n",
    "bertconfig = BertConfig()\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b156c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze pre-trained layers in BERT\n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize multihead attention sarcasm model with BERT embedder\n",
    "\n",
    "# embed_dim = 768 if using bert_base, 1024 for bert_large\n",
    "mh_sarcasm_model = multihead_attn_bert(bert, embed_dim=768, num_attn_layers=3, num_heads=8)\n",
    "mh_sarcasm_model.to(device)\n",
    "\n",
    "# save untrained model weights\n",
    "torch.save(mh_sarcasm_model.state_dict(), \"/projectnb/dl523/students/kjv/EC523_Project/Saved_Models/Multihead_BERT/untrained_mhbert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c77",
   "metadata": {},
   "source": [
    "## Headlines Data Import and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in headlines data\n",
    "df = pd.read_json(DATA_DIR + \"News Headlines/Sarcasm_Headlines_Dataset_v2.json\",lines = True)\n",
    "df = df.rename(columns={'is_sarcastic': 'label'})\n",
    "df = df.drop('article_link', 1)\n",
    "df.head()\n",
    "\n",
    "# splits for headlines training, test, and validation\n",
    "\n",
    "train_headlines, temporary_text, train_label, temporary_label = train_test_split(df['headline'], df['label'], \n",
    "                                                                    random_state=200, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_headlines, test_headlines, validation_label, test_label = train_test_split(temporary_text, temporary_label, \n",
    "                                                                    random_state=200, \n",
    "                                                                    test_size=0.5, \n",
    "                                                                    stratify=temporary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ce262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max length for padding/clipping during tokenization\n",
    "max_length = 35\n",
    "\n",
    "# create tokenized training, validation, and test splits\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_tokens = tokenizer.__call__(train_headlines.tolist(),max_length = max_length,padding = True,truncation = True)\n",
    "validation_tokens = tokenizer.__call__(validation_headlines.tolist(),max_length = max_length,padding = True,truncation = True)\n",
    "test_tokens = tokenizer.__call__(test_headlines.tolist(),max_length = max_length,padding= True,truncation = True)\n",
    "\n",
    "# Stacking the inputs as tensors for use in the BERT model\n",
    "\n",
    "training_set = TensorDataset(torch.tensor(training_tokens['input_ids']),torch.tensor(training_tokens['attention_mask']),torch.tensor(train_label.tolist()))\n",
    "validation_set = TensorDataset(torch.tensor(validation_tokens['input_ids']),torch.tensor(validation_tokens['attention_mask']),torch.tensor(validation_label.tolist()))\n",
    "test_set = TensorDataset(torch.tensor(test_tokens['input_ids']),torch.tensor(test_tokens['attention_mask']),torch.tensor(test_label.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c3778",
   "metadata": {},
   "source": [
    "## Training for Headlines Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders for the headlines sets\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = DataLoader(training_set, batch_size = batch_size, num_workers=2, shuffle = True)\n",
    "validationloader = DataLoader(validation_set, batch_size = batch_size, num_workers=2, shuffle = True)\n",
    "testloader = DataLoader(test_set, batch_size = batch_size, num_workers=2, shuffle = True)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mh_sarcasm bert\n",
    "num_epochs = 50\n",
    "\n",
    "# optimizer using learning rate from multihead reference paper\n",
    "tuning_parameters = [parameter for parameter in mh_sarcasm_model.parameters() if parameter.requires_grad]\n",
    "optimizer = torch.optim.Adam(tuning_parameters,lr = 1e-4)\n",
    "\n",
    "# train the model with early stopping (add scheduler once done)\n",
    "train_mh_bert_headlines(mh_sarcasm_model, trainloader, validationloader, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19238ef0",
   "metadata": {},
   "source": [
    "## Testing on Headlines Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c017f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with lowest validation loss\n",
    "mh_sarcasm_model.load_state_dict(torch.load(\"/projectnb/dl523/students/kjv/EC523_Project/Saved_Models/Multihead_BERT/trained_MH_BERT_headlines.pth\"))\n",
    "\n",
    "# test mh_sarcasm_bert\n",
    "acc = test_mh_bert_headlines(mh_sarcasm_model, testloader)\n",
    "print(f\"Accuracy of network: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2894bb",
   "metadata": {},
   "source": [
    "## Reddit Data Import and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed144eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Reddit data classes and functions\n",
    "import importlib.util\n",
    "fxns_filepath = \"/projectnb/dl523/students/kjv/EC523_Project/sarcasm-detector/reddit_sarcasm/reddit_bert_functions.py\"\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"reddit_bert_functions\", fxns_filepath)\n",
    "reddit_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(reddit_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/projectnb/dl523/students/kjv/EC523_Project/Data/Sarcasm_Reddit/train-balanced-sarcasm.csv\"\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = reddit_mod.split_reddit_data(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e624537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count per each sample for determining max length for tokenization\n",
    "count = x_train.str.split().str.len()\n",
    "plt.hist(count, bins=30, range=(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the reddit data\n",
    "max_length = 35  #based on word count bar plot above, 35 is reasonable\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "reddit_train = reddit_mod.Reddit(x_train, y_train, tokenizer, max_length)\n",
    "reddit_val = reddit_mod.Reddit(x_val, y_val, tokenizer, max_length)\n",
    "reddit_test = reddit_mod.Reddit(x_test, y_test, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d017e4e",
   "metadata": {},
   "source": [
    "## Training for Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Dataloaders\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "trainloader, validationloader, testloader = reddit_mod.get_data_loaders(reddit_train, reddit_val, reddit_test, batch_size, num_workers)\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271367b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "# optimizer using learning rate from multihead reference paper\n",
    "tuning_parameters = [parameter for parameter in mh_sarcasm_model.parameters() if parameter.requires_grad]\n",
    "optimizer = torch.optim.Adam(tuning_parameters,lr = 1e-4)\n",
    "\n",
    "# set mh_sarcasm_model to pre-training weights\n",
    "mh_sarcasm_model.load_state_dict(torch.load(\"/projectnb/dl523/students/kjv/EC523_Project/Saved_Models/Multihead_BERT/untrained_mhbert.pth\"))\n",
    "\n",
    "# train the model with early stopping (add scheduler once done)\n",
    "train_mh_bert_reddit(mh_sarcasm_model, trainloader, validationloader, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2088593",
   "metadata": {},
   "source": [
    "## Testing on Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b867bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with lowest validation loss\n",
    "mh_sarcasm_model.load_state_dict(torch.load(\"/projectnb/dl523/students/kjv/EC523_Project/Saved_Models/Multihead_BERT/trained_MH_BERT_reddit.pth\"))\n",
    "\n",
    "# test mh_sarcasm_bert\n",
    "acc = test_mh_bert_reddit(mh_sarcasm_model, testloader)\n",
    "print(f\"Accuracy of network: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d370ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
