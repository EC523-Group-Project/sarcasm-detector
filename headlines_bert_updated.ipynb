{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EC523-Group-Project/sarcasm-detector/blob/main/headlines_bert_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuBOG76lSZpr"
      },
      "outputs": [],
      "source": [
        "#  !pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxn3yq30RsU_"
      },
      "source": [
        "#523 Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5PH5gxUSp9K"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    print('transformers not installed')\n",
        "    print('Installing now...')\n",
        "    !pip install -q git+https://github.com/huggingface/transformers.git\n",
        "    pass  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJkBa31eRq7s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import io \n",
        "import os\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "import transformers\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers.utils.dummy_pt_objects import AutoModelForSequenceClassification\n",
        "from transformers import AutoModelForTokenClassification,AutoConfig, AutoModel,AutoTokenizer,BertModel,BertConfig,AdamW, get_constant_schedule,BertForSequenceClassification,get_linear_schedule_with_warmup\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "#Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dlHAyT4T5NS"
      },
      "outputs": [],
      "source": [
        "colab = False\n",
        "if colab == True:\n",
        "    #Mounting Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    \n",
        "    %cd '/content/gdrive/Shareddrives/523 Project/Data'\n",
        "    %ls\n",
        "else:\n",
        "    DATA_DIR = '/projectnb2/dl523/students/colejh/pr'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YESWPLT9WqkE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# with open('Sarcasm_Headlines_Dataset_v2.json') as f:\n",
        "#     headlines = json.loads(\"[\" + \n",
        "#         f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
        "#     \"]\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxEUaAR5eceH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# label = []\n",
        "# hd = []\n",
        "\n",
        "# for i in range(len(headlines)):\n",
        "#   label.append(headlines[i]['is_sarcastic'])\n",
        "#   hd.append(headlines[i]['headline'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJEbsH0fTkgg"
      },
      "outputs": [],
      "source": [
        "#Class to store the headline articles for use with sequentialbert\n",
        "\n",
        "class Headlines(Dataset):\n",
        "\n",
        "  def __init__(self,filepath,selected_tokenizer,max_length=None):\n",
        "\n",
        "    headlines = []\n",
        "    labels = []\n",
        "\n",
        "    #potentially assign validation and test splits within this class\n",
        "\n",
        "    \n",
        "    # Funky JSON file, need to make slight adjustments reading it in \n",
        "    with open(str(filepath)) as f:\n",
        "      data = json.loads(\"[\" + \n",
        "          f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
        "      \"]\")\n",
        "\n",
        "    #Store text and label information   \n",
        "    for i in range(len(data)):\n",
        "      labels.append(data[i]['is_sarcastic'])\n",
        "      headlines.append(data[i]['headline'])\n",
        "\n",
        "    self.length = len(headlines)\n",
        "\n",
        "    if max_length == None:\n",
        "      max_length = selected_tokenizer.model_max_length\n",
        "    else:\n",
        "      max_length = max_length\n",
        "\n",
        "    #Create the tokenized version of our headlines\n",
        "\n",
        "    self.inputs = selected_tokenizer(headlines,padding = True,truncation = True, add_special_tokens = True,return_tensors='pt')\n",
        "    label_tensors = {'labels':torch.tensor(labels)}\n",
        "    self.inputs.update(label_tensors)\n",
        "\n",
        "    return\n",
        "\n",
        "  #Required for the token's being passed to the transformer \n",
        "  def __getitem__(self,item):\n",
        "    return {key: self.inputs[key][item] for key in self.inputs.keys()}\n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F_NRAtnrQq1"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Defining the pretrained model (BERT)\n",
        "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
        "selected_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "bert_model = BertForSequenceClassification(config)\n",
        "\n",
        "#cuda\n",
        "bert_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXQlN2vadph9"
      },
      "outputs": [],
      "source": [
        "#Training data (need to split the data into train test validation, either can do this manually \n",
        "#in seperate folders, or assign splits randomly shuffled within the headlines class)\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "headlines_data = Headlines('Sarcasm_Headlines_Dataset_v2.json',selected_tokenizer = selected_tokenizer,max_length = 30)\n",
        "trainloader = DataLoader(headlines_data, batch_size = batch_size, shuffle = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOCptQXI3XFo"
      },
      "outputs": [],
      "source": [
        "print(headlines_data[1]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l937PRQjsxi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Optimizer and training\n",
        "Epochs = 1\n",
        "optimizer = AdamW(bert_model.parameters(),lr = 1e-3,eps = 1e-6)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = len(trainloader)*Epochs)\n",
        "\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "final_loss = 0\n",
        "\n",
        "bert_model.train()\n",
        "\n",
        "for epoch in range(1, Epochs+1):\n",
        "  for batch in tqdm(trainloader,total = len(trainloader)):\n",
        "\n",
        "    labels.extend(batch['labels'].numpy().flatten().tolist())\n",
        "    # for key,value in batch.items():\n",
        "    #   batch = {key:value.type(torch.long).to(device)}\n",
        "    batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n",
        "\n",
        "\n",
        "    bert_model.zero_grad()\n",
        "\n",
        "    outputs = bert_model(**batch)\n",
        "\n",
        "    loss,logits = outputs[:2]\n",
        "\n",
        "    final_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions.extend(logits.argmax(axis = -1).flatten().tolist())\n",
        "\n",
        "  average_loss = final_loss/len(trainloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyCa6M0sLCEL"
      },
      "outputs": [],
      "source": [
        "p = np.array(predictions)\n",
        "l = np.array(labels)\n",
        "j = p == l\n",
        "print(sum(j)/len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJGZfEURBZ59"
      },
      "outputs": [],
      "source": [
        "#Looking at the tokenizer and its output\n",
        "new = selected_tokenizer(\"You are teh smartest in the world, absolutely\",return_tensors=\"pt\")\n",
        "new.to(device)\n",
        "print(new)\n",
        "outputs = bert_model(**new)\n",
        "print(outputs)\n",
        "test = torch.nn.functional.softmax(outputs.logits, dim = 1)\n",
        "print(test)\n",
        "bert_model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5chIZ8qYBZ5-"
      },
      "source": [
        "##Pretrained BERT as input to downstream layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1jiyzKUgyVc"
      },
      "outputs": [],
      "source": [
        "#Getting the BERT outputs for use with other downstream model implementations \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8UPTpUABZ5-"
      },
      "outputs": [],
      "source": [
        "#Reading in the data\n",
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\",lines = True)\n",
        "df = df.rename(columns={'is_sarcastic': 'label'})\n",
        "df = df.drop('article_link', 1)\n",
        "df.head()\n",
        "\n",
        "#splits for training test validation\n",
        "\n",
        "train_headlines, temporary_text, train_label, temporary_label = train_test_split(df['headline'], df['label'], \n",
        "                                                                    random_state=200, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "validation_headlines, test_headlines, validation_label, test_label = train_test_split(temporary_text, temporary_label, \n",
        "                                                                    random_state=200, \n",
        "                                                                    test_size=0.5, \n",
        "                                                                    stratify=temporary_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q033oNHxBZ5-",
        "outputId": "da085589-9a0b-4343-c4df-058772dce3e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "bert_for_sarcasm(\n",
              "  (input_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (linear3): Linear(in_features=128, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (log): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#initialize the BERT \n",
        "\n",
        "bertconfig = BertConfig()\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "bert2 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#Only want to train the additional layers at first,freezing pretrained\n",
        "for param in bert2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Create a model which takes output from BERT and run through new layers for classification\n",
        "\n",
        "\n",
        "class bert_for_sarcasm(nn.Module):\n",
        "\n",
        "    def __init__(self,input_model):\n",
        "        super(bert_for_sarcasm,self).__init__()\n",
        "        \n",
        "        self.input_model = input_model\n",
        "        \n",
        "        self.linear = nn.Linear(768,256)\n",
        "        \n",
        "        self.linear2 = nn.Linear(256,128)\n",
        "        \n",
        "        self.linear3 = nn.Linear(128,2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.log = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "    def forward(self,input_values,attention_mask):\n",
        "        \n",
        "        _,output = self.input_model(input_values, attention_mask=attention_mask).values()\n",
        "\n",
        "        output = self.linear(output)\n",
        "        \n",
        "        output = self.relu(output)\n",
        "        \n",
        "        output = self.linear2(output)\n",
        "        \n",
        "        output = self.relu(output)\n",
        "        \n",
        "        output = self.linear3(output)\n",
        "        \n",
        "        output = self.log(output)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    \n",
        "#Put updated sarcasm model on GPU\n",
        "sarcasm_model = bert_for_sarcasm(bert2)\n",
        "sarcasm_model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7DF9izXBZ5_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkq9RWyaBZ5_"
      },
      "outputs": [],
      "source": [
        "#Set max length for the padding/clipping\n",
        "count = df['headline'].str.split().str.len()\n",
        "count.describe()\n",
        "\n",
        "max_length = 35\n",
        "batch_size = 16\n",
        "#Create tokenized training, validation, and test splits\n",
        "\n",
        "training_tokens = tokenizer2.batch_encode_plus(train_headlines.tolist(),max_length = max_length,padding = True,truncation = True)\n",
        "validation_tokens = tokenizer2.batch_encode_plus(validation_headlines.tolist(),max_length = max_length,padding = True,truncation = True)\n",
        "test_tokens = tokenizer2.batch_encode_plus(test_headlines.tolist(),max_length = max_length,padding= True,truncation = True)\n",
        "\n",
        "#Stacking the inputs as tensors for use in the BERT model\n",
        "\n",
        "training_set = TensorDataset(torch.tensor(training_tokens['input_ids']),torch.tensor(training_tokens['attention_mask']),torch.tensor(train_label.tolist()))\n",
        "validation_set = TensorDataset(torch.tensor(validation_tokens['input_ids']),torch.tensor(validation_tokens['attention_mask']),torch.tensor(validation_label.tolist()))\n",
        "test_set = TensorDataset(torch.tensor(test_tokens['input_ids']),torch.tensor(test_tokens['attention_mask']),torch.tensor(test_label.tolist()))\n",
        "\n",
        "\n",
        "#Dataloaders for the sets\n",
        "\n",
        "trainloader = DataLoader(training_set, batch_size = batch_size,num_workers=2,shuffle = True)\n",
        "validationloader = DataLoader(validation_set, batch_size = batch_size,num_workers=2,shuffle = True)\n",
        "testloader = DataLoader(test_set, batch_size = batch_size,num_workers=2,shuffle = True)\n",
        "\n",
        "#Loss function (standard in the BERT documentation is MSELoss)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07pcH3SsBZ6A",
        "outputId": "76bbb508-803e-4771-d082-b87191e8ef29",
        "colab": {
          "referenced_widgets": [
            "d178e89f1e934062ab995fd8fa244231",
            "355121056619492fad68eccb045bc383",
            "49c597e21a9b402db2fc2ed3aa98d28d",
            "2725b5bb4986440c947ee4f7f7e4ed0a"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d178e89f1e934062ab995fd8fa244231",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1431.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss:  0.6946685314178467\n",
            "Loss:  0.7624862194061279\n",
            "Loss:  0.5097077488899231\n",
            "Loss:  0.4342424273490906\n",
            "Loss:  0.6400635838508606\n",
            "Loss:  0.45576587319374084\n",
            "Loss:  0.41270604729652405\n",
            "Loss:  0.5740150809288025\n",
            "Loss:  0.6605370044708252\n",
            "Loss:  0.49485713243484497\n",
            "Loss:  0.49915626645088196\n",
            "Loss:  0.5157905220985413\n",
            "Loss:  0.6512148380279541\n",
            "Loss:  0.507512629032135\n",
            "Loss:  0.6528090834617615\n",
            "\n",
            "Epoch:  2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "355121056619492fad68eccb045bc383",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1431.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss:  0.42927053570747375\n",
            "Loss:  0.48973336815834045\n",
            "Loss:  0.42011046409606934\n",
            "Loss:  0.786328136920929\n",
            "Loss:  0.4401737451553345\n",
            "Loss:  0.36362722516059875\n",
            "Loss:  0.4542113244533539\n",
            "Loss:  0.3877626657485962\n",
            "Loss:  0.38293612003326416\n",
            "Loss:  0.3922159969806671\n",
            "Loss:  0.3293704390525818\n",
            "Loss:  0.3334474563598633\n",
            "Loss:  0.5162965655326843\n",
            "Loss:  0.34677645564079285\n",
            "Loss:  0.3888673186302185\n",
            "\n",
            "Epoch:  3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49c597e21a9b402db2fc2ed3aa98d28d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1431.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss:  0.4259621798992157\n",
            "Loss:  0.6958497762680054\n",
            "Loss:  0.2669283449649811\n",
            "Loss:  0.35002878308296204\n",
            "Loss:  0.4710785448551178\n",
            "Loss:  0.5304607152938843\n",
            "Loss:  0.25705429911613464\n",
            "Loss:  0.5202292799949646\n",
            "Loss:  0.4129214286804199\n",
            "Loss:  0.45299890637397766\n",
            "Loss:  0.4684998095035553\n",
            "Loss:  0.40212544798851013\n",
            "Loss:  0.37286877632141113\n",
            "Loss:  0.5327225923538208\n",
            "Loss:  0.21048025786876678\n",
            "\n",
            "Epoch:  4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2725b5bb4986440c947ee4f7f7e4ed0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1431.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss:  0.5896018743515015\n",
            "Loss:  0.2294345647096634\n",
            "Loss:  0.46674901247024536\n",
            "Loss:  0.43182530999183655\n",
            "Loss:  0.4262551963329315\n",
            "Loss:  0.4975981116294861\n",
            "Loss:  0.3880462348461151\n",
            "Loss:  0.48125943541526794\n",
            "Loss:  0.2505165934562683\n",
            "Loss:  0.37281274795532227\n",
            "Loss:  0.3356032073497772\n",
            "Loss:  0.4041266441345215\n",
            "Loss:  0.5346697568893433\n",
            "Loss:  0.2478064000606537\n",
            "Loss:  0.43881955742836\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training sarcasm bert\n",
        "Epochs = 4\n",
        "\n",
        "#optimizer and scheduler for learning rate\n",
        "optimizer = AdamW(sarcasm_model.parameters(),lr = 1e-3,eps = 1e-6)\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = len(trainloader)*Epochs)\n",
        "\n",
        "loss_acc = 0 \n",
        "sarcasm_model.train()\n",
        "for epoch in range(1, Epochs+1):\n",
        "    print('Epoch: ',epoch)\n",
        "    for idx, (inputs,attention_mask,label) in enumerate(tqdm(trainloader,total = len(trainloader))):\n",
        "\n",
        "        inputs,attention_mask,label = inputs.to(device),attention_mask.to(device),label.to(device)\n",
        "\n",
        "        sarcasm_model.zero_grad()\n",
        "\n",
        "        output = sarcasm_model(inputs,attention_mask)\n",
        "\n",
        "        loss = loss_function(output,label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "#         scheduler.step()\n",
        "        loss_acc +=loss.item()\n",
        "        if idx%100 == 0:\n",
        "            print('Loss: ',float(loss))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxzWoUqtBZ6A",
        "outputId": "eee47ea0-87b5-4b98-cbe2-acc4d07d4866",
        "colab": {
          "referenced_widgets": [
            "826fd3b81e954d55b026187d3cd75e98"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "826fd3b81e954d55b026187d3cd75e98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=179.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "correct = []\n",
        "final_pred = []\n",
        "final_lab = []\n",
        "with torch.no_grad():\n",
        "    \n",
        "    for idx, (inputs,attention_mask,label) in enumerate(tqdm(testloader,total = len(testloader))):\n",
        "        inputs,attention_mask,label = inputs.to(device),attention_mask.to(device), label.to(device)\n",
        "        output = sarcasm_model(inputs,attention_mask).cpu()\n",
        "        preds = output.data.max(1, keepdim=True)[1].squeeze(1).numpy()\n",
        "\n",
        "        l = label.cpu().numpy()\n",
        "        comp = l == preds\n",
        "        final_lab.extend(l)\n",
        "        final_pred.extend(preds)\n",
        "        for i in range(l.size):\n",
        "          if comp[i] == True:\n",
        "            correct.append(1)\n",
        "          else:\n",
        "            correct.append(0)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAZIuUTYBZ6A",
        "outputId": "1b5dde62-52bc-41f9-db04-30fd1f20ff82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mPrecision, Recall and Accuracy for Headline Data:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Sarcastic       0.78      0.83      0.80      1499\n",
            "Not Sarcastic       0.80      0.74      0.77      1363\n",
            "\n",
            "     accuracy                           0.79      2862\n",
            "    macro avg       0.79      0.78      0.79      2862\n",
            " weighted avg       0.79      0.79      0.79      2862\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Confusion Matrix\n",
        "conf_mat = confusion_matrix(final_lab, final_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
        "fig, ax = plt.subplots(figsize=(20,20))\n",
        "plt.title('Confusion Matrix For Sarcasm_BERT')\n",
        "disp.plot(ax = ax)\n",
        "\n",
        "r_words = [\"Sarcastic\",\"Not Sarcastic\"]\n",
        "class_report = classification_report(final_lab,final_pred,target_names =r_words)\n",
        "print('\\033[1m'+'Precision, Recall and Accuracy for Headline Data:\\n')\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTFBJIbNBZ6B"
      },
      "outputs": [],
      "source": [
        "# #Optimizer and training\n",
        "# Epochs = 1\n",
        "# optimizer = AdamW(bert_model.parameters(),lr = 1e-3,eps = 1e-6)\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = len(trainloader)*Epochs)\n",
        "\n",
        "# predictions = []\n",
        "# labels = []\n",
        "\n",
        "# final_loss = 0\n",
        "\n",
        "# bert_model.train()\n",
        "\n",
        "# for epoch in range(1, Epochs+1):\n",
        "#   for batch in tqdm(trainloader,total = len(trainloader)):\n",
        "\n",
        "#     labels.extend(batch['labels'].numpy().flatten().tolist())\n",
        "#     # for key,value in batch.items():\n",
        "#     #   batch = {key:value.type(torch.long).to(device)}\n",
        "#     batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n",
        "\n",
        "\n",
        "#     bert_model.zero_grad()\n",
        "\n",
        "#     outputs = bert_model(batch['input_ids'],batch['attention_mask'])\n",
        "#     print(len(outputs[1][1]))\n",
        "#     loss,logits = outputs[:2]\n",
        "\n",
        "#     final_loss += loss.item()\n",
        "\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     scheduler.step()\n",
        "\n",
        "#     logits = logits.detach().cpu().numpy()\n",
        "\n",
        "#     predictions.extend(logits.argmax(axis = -1).flatten().tolist())\n",
        "\n",
        "#   average_loss = final_loss/len(trainloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "headlines_bert_updated.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}