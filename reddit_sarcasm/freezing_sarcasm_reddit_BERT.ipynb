{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b9265b-c829-4c69-94a6-b44e077058fb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7feebd-eee4-434f-b7e7-8150f2902af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ImportError as e:\n",
    "    print('transformers not installed')\n",
    "    print('Installing now...')\n",
    "    !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd30d18f-22d0-408d-a252-078f49c77aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import reddit_bert_functions as fun\n",
    "from bert_sarcasm_model import bert_for_sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8114864-e656-41fd-932a-ba647209e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import transformers\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.utils.dummy_pt_objects import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForTokenClassification,AutoConfig, AutoModel,AutoTokenizer,BertModel,BertConfig,AdamW, get_constant_schedule,BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1600c-0b4b-4089-b37d-22ae8edd6713",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d883b762-b187-49ed-806b-3b756724764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_save_dir = \"/projectnb/dl523/students/nannkat/Project/training/cp_freeze.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2e13d-e69d-4f65-9efa-4283510ff49a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed625ce8-f1f4-4516-95a8-682076d2bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'train-balanced-sarcasm.csv'\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = fun.split_reddit_data(csv_path)\n",
    "\n",
    "max_length = 35  #based on word count bar plot above, 35 is reasonable\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "reddit_train = fun.Reddit(x_train, y_train, tokenizer, max_length)\n",
    "reddit_val = fun.Reddit(x_val, y_val, tokenizer, max_length)\n",
    "reddit_test = fun.Reddit(x_test, y_test, tokenizer, max_length)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "trainloader, validationloader, testloader = fun.get_data_loaders(reddit_train, reddit_val, reddit_test, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a13f7-e6c6-4d0a-8539-2dd687d310b2",
   "metadata": {},
   "source": [
    "## Freeze tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999859a6-e0de-45e2-ad3e-e099a36a99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_tuning(trainloader, validationloader, layer_counts, epoch_count, batch_size, learning_rate, device):\n",
    "    print(device)\n",
    "    ##loop over layer idx\n",
    "    freeze_losses = []\n",
    "    best_loss = float('inf')\n",
    "    best_count = 0\n",
    "    for i, num_layers in enumerate(layer_counts):\n",
    "        \n",
    "        print(\"Test {}/{}\".format(i+1, len(layer_counts)))\n",
    "        print(\"Number of layers to be unfrozen: {}\".format(num_layers))\n",
    "        print()\n",
    "        \n",
    "        #create new model/freeze appropriate\n",
    "        bertconfig = BertConfig()\n",
    "        bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        fun.freeze_by_children(bert, num_layers)\n",
    "        sarcasm_model = bert_for_sarcasm(bert)\n",
    "    \n",
    "        #train over x epochs\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(device)\n",
    "        loss_function = nn.BCELoss()\n",
    "        sarcasm_model.to(device)\n",
    "        losses, val_losses = fun.train_reddit(sarcasm_model, trainloader, validationloader, epoch_count, \n",
    "                                              batch_size, device, lr = learning_rate, \n",
    "                                              model_save_dir = checkpoint_save_dir)\n",
    "        \n",
    "        curr_loss = min(val_losses)\n",
    "        \n",
    "        \n",
    "        #add loss to losses array and update best if it beats best\n",
    "        freeze_losses.append(curr_loss)\n",
    "        if curr_loss < best_loss:\n",
    "            best_loss = curr_loss\n",
    "            best_count = num_layers\n",
    "            \n",
    "        print(\"Training done!\")\n",
    "        print(\"Loss for {} layers is {}. Best loss is {} for {} layers\".format(num_layers, round(curr_loss, 4), \n",
    "                                                                               round(best_loss, 4), best_count))\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    return freeze_losses, best_loss, best_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456945ff-3dc7-45b3-8ec4-ffa63672b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Test 2/4\n",
      "Number of layers to be unfrozen: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input model has 12 encoding layers\n",
      "The model has 1 pooling layers\n",
      "Bert layer 11 has been unfrozen\n",
      "Bert layer 12 has been unfrozen\n",
      "Pooling layer has been unfrozen\n",
      "cuda\n",
      "Epoch:  1\n",
      "Elapsed [0:00:00], Iteration [1/12635]Loss: 0.6924\n",
      "Elapsed [0:02:55], Iteration [2001/12635]Loss: 0.4904\n",
      "Elapsed [0:05:50], Iteration [4001/12635]Loss: 0.6312\n",
      "Elapsed [0:08:45], Iteration [6001/12635]Loss: 0.4399\n",
      "Elapsed [0:11:40], Iteration [8001/12635]Loss: 0.5953\n",
      "Elapsed [0:14:35], Iteration [10001/12635]Loss: 0.6319\n",
      "Elapsed [0:17:30], Iteration [12001/12635]Loss: 0.5000\n",
      "Validating.....\n",
      "Decrease in validation loss. Early stop counter reset to 0.\n",
      "New lowest loss, saving model...\n",
      "Model checkpoint saved to /projectnb/dl523/students/nannkat/Project/training/cp_freeze.ckpt\n",
      "Epoch 1. Training accuracy: 0.7237. Validation accuracy: 0.7515.\n",
      "\n",
      "Epoch:  2\n",
      "Elapsed [0:19:29], Iteration [1/12635]Loss: 0.5239\n",
      "Elapsed [0:22:24], Iteration [2001/12635]Loss: 0.4913\n",
      "Elapsed [0:25:19], Iteration [4001/12635]Loss: 0.4758\n",
      "Elapsed [0:28:14], Iteration [6001/12635]Loss: 0.6185\n",
      "Elapsed [0:31:09], Iteration [8001/12635]Loss: 0.6206\n",
      "Elapsed [0:34:04], Iteration [10001/12635]Loss: 0.4677\n",
      "Elapsed [0:36:59], Iteration [12001/12635]Loss: 0.4931\n",
      "Validating.....\n",
      "Decrease in validation loss. Early stop counter reset to 0.\n",
      "New lowest loss, saving model...\n",
      "Model checkpoint saved to /projectnb/dl523/students/nannkat/Project/training/cp_freeze.ckpt\n",
      "Epoch 2. Training accuracy: 0.7559. Validation accuracy: 0.7637.\n",
      "\n",
      "Epoch:  3\n",
      "Elapsed [0:38:58], Iteration [1/12635]Loss: 0.4409\n",
      "Elapsed [0:41:54], Iteration [2001/12635]Loss: 0.4997\n",
      "Elapsed [0:44:49], Iteration [4001/12635]Loss: 0.4707\n",
      "Elapsed [0:47:43], Iteration [6001/12635]Loss: 0.6171\n",
      "Elapsed [0:50:38], Iteration [8001/12635]Loss: 0.5530\n",
      "Elapsed [0:53:33], Iteration [10001/12635]Loss: 0.3546\n",
      "Elapsed [0:56:28], Iteration [12001/12635]Loss: 0.3910\n",
      "Validating.....\n",
      "Decrease in validation loss. Early stop counter reset to 0.\n",
      "New lowest loss, saving model...\n",
      "Model checkpoint saved to /projectnb/dl523/students/nannkat/Project/training/cp_freeze.ckpt\n",
      "Epoch 3. Training accuracy: 0.7697. Validation accuracy: 0.7684.\n",
      "\n",
      "Epoch:  4\n",
      "Elapsed [0:58:28], Iteration [1/12635]Loss: 0.4335\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-5\n",
    "layer_counts = [2 , 3 , 6 , 12]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "freeze_losses, best_loss, best_count = freeze_tuning(trainloader, validationloader, layer_counts,\n",
    "                                                     epoch_count = epochs, batch_size = batch_size, learning_rate = lr,\n",
    "                                                    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b261a-bb87-45b0-bd88-bb8587ca098b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
